{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pos_proc.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNA89iuh1+72pSu25up90xR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/custodif/IFT6135H20_assignment/blob/master/pos_proc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLU6zB-QuPWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import nltk\n",
        "import re \n",
        "\n",
        "def truncate(content, max_words):\n",
        "    if len(content.split()) <= max_words:\n",
        "        return content\n",
        "    else:\n",
        "        return ' '.join(content.split(' ', max_words)[0:-1])\n",
        "\n",
        "def remove_repeated_bigr(content, max_bigram = 4, max_words=1):\n",
        "    seen = []\n",
        "    seen_word = []\n",
        "    result = []\n",
        "\n",
        "    text = content.split()\n",
        "    bigrams = nltk.bigrams(text)\n",
        "\n",
        "    for index, item in enumerate(bigrams):\n",
        "        if item not in seen:\n",
        "            seen.append(item)\n",
        "            for word in item:\n",
        "                if (word not in seen_word):\n",
        "                   result.append(word)\n",
        "                   seen_word.append(word)\n",
        "                   if (len(seen_word) > (max_words)): \n",
        "                      seen_word.pop(0)\n",
        "        if (len(seen) > (max_bigram)): \n",
        "            seen.pop(0)\n",
        "\n",
        "    sentence = ' '.join(result)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "path_source = '/content/predictions.txt'\n",
        "\n",
        "lines_source = io.open(path_source, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "lines_output = []\n",
        "\n",
        "for index, l in enumerate(lines_source):\n",
        "    l_removed = remove_repeated_bigr(l, max_bigram=4, max_words=1)\n",
        "    l_truncated = truncate(l_removed, max_words=45)\n",
        "    # if l != l_truncated:\n",
        "    #    print(l)\n",
        "    #    print(l_truncated)\n",
        "    lines_output.append(l_truncated)    \n",
        "\n",
        "with open('/content/predictions_truncated.txt', 'w') as f:\n",
        "  for item in lines_output:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cF8wksBU3-ND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}